{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the reviews from Google Maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the GOOGLE_PLACES_API_KEY environment variable\n",
    "os.environ[\"GOOGLE_PLACES_API_KEY\"] = \"AIzaSyD_NUIC2KPfD-8euMjdKgpBsLB05MFzSgE\"\n",
    "\n",
    "def fetch_reviews(place_id, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\n",
    "        'place_id': place_id,\n",
    "        \"X-Goog-FieldMask\": \"places.reviews\",\n",
    "        'key': api_key,\n",
    "    }\n",
    "    print(f\"Making API request to URL: {url} with query parameters: {params}\")\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception if the status code indicates an error occurred\n",
    "        data = response.json()\n",
    "        return data.get('result', {}).get('reviews', [])\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        print(\"Please check your API key and usage limits.\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error occurred: {err}\")\n",
    "    return []\n",
    "\n",
    "place_id = \"ChIJrTH0vwYnQg0RwFJPB0NzKeE\"  # Example place ID\n",
    "api_key = os.environ[\"GOOGLE_PLACES_API_KEY\"]\n",
    "reviews = fetch_reviews(place_id, api_key)\n",
    "\n",
    "if reviews:\n",
    "    print(\"Reviews fetched successfully:\")\n",
    "    for review in reviews:\n",
    "        print(f\"Author: {review.get('author_name')}, Rating: {review.get('rating')}, Review: {review.get('text')}\")\n",
    "else:\n",
    "    print(\"Error: Unable to fetch reviews from the API\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the better one and the second one to compare what they have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews for Talent Garden Madrid:\n",
      "Cold and very noisy. Phone booths are open spaces between 4 walls, but no ceiling. Useless\n",
      "Decided to visit on my week trip to Madrid. At first I was a little skeptical: thought it would be crowded, noisy, and rude staff. BUT I was wrong. This is such a pleasant place as I was welcomed the moment I walked through the door. Marta and Alexa (I think) were so gracious and helped me get situated for the day. Allowed me to sit and purchase a day pass since it wasn't working online. Marta checked on me through the day and informed me of the office space, private rooms for calls, cafe etc. The guy running the cafe was also very pleasant.\n",
      "\n",
      "The internet was super fast - which is very important. The table/sear selection were comfortable, and the outlets are easily accessible.\n",
      "\n",
      "I will definitely return on my visits and send people their way.\n",
      "Thank you for a great experience!\n",
      "Nice offices walking distance to centre of Madrid.\n",
      "\n",
      "Clean, Spacious, Modern, plenty of room for expansion, some nice bars near by too.\n",
      "Close to atocha\n",
      "Nice day light\n",
      "As a current member, this is an excellent coworking spot. There are multiple spaces in the building for different needs. The staff are very kind and accommodating. I definitely recommend coming by to check it out!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load the data from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"TalentReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print the reviews for the place\n",
    "print(\"Reviews for Talent Garden Madrid:\")\n",
    "\n",
    "for review in data.get(\"result\", {}).get(\"reviews\", []):\n",
    "    print(review.get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review sentiment: negative\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the VADER sentiment analysis model from NLTK\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Define the sentiment analysis function\n",
    "def analyze_sentiment(text):\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    if sentiment['compound'] > 0.05:\n",
    "        return 'positive'\n",
    "    elif sentiment['compound'] < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Load the reviews from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"TalentReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Classify the reviews as positive, negative, or neutral\n",
    "for review in data.get(\"result\", {}).get(\"reviews\", []):\n",
    "    sentiment = analyze_sentiment(review.get(\"text\"))\n",
    "    print(f\"Review sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sentiment for the first one is consider positive in almost all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in the reviews:\n",
      "day: 4\n",
      "space: 3\n",
      "nice: 3\n",
      "noisi: 2\n",
      "visit: 2\n",
      "madrid: 2\n",
      "staff: 2\n",
      "pleasant: 2\n",
      "walk: 2\n",
      "marta: 2\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk.corpus\n",
    "\n",
    "# Download the NLTK English tokenizer and stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(stemmer.stem(word) for word in words)\n",
    "\n",
    "# Load the reviews from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"TalentReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the text of the reviews\n",
    "reviews = [preprocess(review.get(\"text\")) for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "\n",
    "# Count the frequency of each word in the preprocessed reviews\n",
    "word_counts = collections.Counter(word for review in reviews for word in review.split())\n",
    "\n",
    "# Print the most common words in the reviews\n",
    "print(\"Most common words in the reviews:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API request to URL: https://maps.googleapis.com/maps/api/place/details/json with query parameters: {'place_id': 'ChIJCVUBCA4mQg0RbIccVWP9JA8', 'key': 'AIzaSyD_NUIC2KPfD-8euMjdKgpBsLB05MFzSgE'} and headers: {'X-Goog-FieldMask': 'places.reviews'}\n",
      "Data saved as FreelandReviews.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the GOOGLE_PLACES_API_KEY environment variable\n",
    "os.environ[\"GOOGLE_PLACES_API_KEY\"] = \"AIzaSyD_NUIC2KPfD-8euMjdKgpBsLB05MFzSgE\"\n",
    "\n",
    "def fetch_reviews(place_id, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    headers = {\n",
    "        \"X-Goog-FieldMask\": \"places.reviews\"\n",
    "    }\n",
    "    params = {\n",
    "        'place_id': place_id,\n",
    "        'key': api_key,\n",
    "    }\n",
    "    print(f\"Making API request to URL: {url} with query parameters: {params} and headers: {headers}\")\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception if the status code indicates an error occurred\n",
    "        data = response.json()\n",
    "        return data\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        print(\"Please check your API key and usage limits.\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error occurred: {err}\")\n",
    "    return None\n",
    "\n",
    "place_id = \"ChIJCVUBCA4mQg0RbIccVWP9JA8\"\n",
    "api_key = os.environ[\"GOOGLE_PLACES_API_KEY\"]\n",
    "data = fetch_reviews(place_id, api_key)\n",
    "\n",
    "if data:\n",
    "    # Save the data as a JSON file\n",
    "    with open(os.path.join(os.getcwd(), \"FreelandReviews.json\"), \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(\"Data saved as FreelandReviews.json\")\n",
    "else:\n",
    "    print(\"Error: Unable to fetch reviews from the API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "are the by to definitely staff this as spaces very\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "nice by to clean some centre modern near room expansion\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "nice light close atocha day to by bars plenty expansion\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "the and was to for me day on of very\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "noisy but useless phone ceiling open booths cold no walls\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_ for token in doc]\n",
    "    cleaned_tokens = [token for token in tokens if token and token not in stop_words and token.isalpha()]\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Load the reviews from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"TalentReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the text of the reviews\n",
    "reviews = [review.get(\"text\") for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "\n",
    "# Skip preprocessing if the data is already preprocessed\n",
    "if all(isinstance(review, str) for review in reviews):\n",
    "    cleaned_reviews = reviews\n",
    "else:\n",
    "    cleaned_reviews = [preprocess(review) for review in reviews]\n",
    "\n",
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the cleaned_reviews into a matrix of token counts\n",
    "review_matrix = vectorizer.fit_transform(cleaned_reviews)\n",
    "\n",
    "# Get the list of feature names after fitting the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a LatentDirichletAllocation model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "# Fit the model to the review_matrix\n",
    "lda.fit(review_matrix)\n",
    "\n",
    "# Display the topics\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {i}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[-10:][::-1]]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(review):\n",
    "    # Convert the review into lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Tokenize the review into individual words\n",
    "    words = word_tokenize(review)\n",
    "    \n",
    "    # Remove punctuation and stopwords\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"nice\" + 0.010*\"day\" + 0.010*\"atocha\" + 0.010*\"light\" + 0.010*\"close\" + 0.010*\"space\" + 0.010*\"booth\" + 0.010*\"useless\" + 0.010*\"wall\" + 0.010*\"ceiling\"')\n",
      "(1, '0.046*\"noisy\" + 0.046*\"ceiling\" + 0.046*\"booth\" + 0.046*\"cold\" + 0.046*\"phone\" + 0.046*\"wall\" + 0.046*\"open\" + 0.046*\"useless\" + 0.046*\"space\" + 0.046*\"light\"')\n",
      "(2, '0.010*\"day\" + 0.010*\"visit\" + 0.010*\"cafe\" + 0.010*\"room\" + 0.010*\"space\" + 0.010*\"pleasant\" + 0.010*\"marta\" + 0.010*\"noisy\" + 0.010*\"walked\" + 0.010*\"close\"')\n",
      "(3, '0.010*\"day\" + 0.010*\"light\" + 0.010*\"atocha\" + 0.010*\"nice\" + 0.010*\"space\" + 0.010*\"noisy\" + 0.010*\"close\" + 0.010*\"booth\" + 0.010*\"useless\" + 0.010*\"open\"')\n",
      "(4, '0.010*\"day\" + 0.010*\"cafe\" + 0.010*\"visit\" + 0.010*\"pleasant\" + 0.010*\"marta\" + 0.010*\"way\" + 0.010*\"purchase\" + 0.010*\"pas\" + 0.010*\"thank\" + 0.010*\"first\"')\n",
      "(5, '0.010*\"nice\" + 0.010*\"day\" + 0.010*\"close\" + 0.010*\"atocha\" + 0.010*\"light\" + 0.010*\"space\" + 0.010*\"booth\" + 0.010*\"madrid\" + 0.010*\"open\" + 0.010*\"noisy\"')\n",
      "(6, '0.083*\"nice\" + 0.044*\"room\" + 0.044*\"office\" + 0.044*\"madrid\" + 0.044*\"distance\" + 0.044*\"clean\" + 0.044*\"centre\" + 0.044*\"bar\" + 0.044*\"modern\" + 0.044*\"expansion\"')\n",
      "(7, '0.038*\"day\" + 0.026*\"marta\" + 0.026*\"visit\" + 0.026*\"pleasant\" + 0.026*\"cafe\" + 0.014*\"walked\" + 0.014*\"door\" + 0.014*\"trip\" + 0.014*\"gracious\" + 0.014*\"selection\"')\n",
      "(8, '0.010*\"day\" + 0.010*\"pleasant\" + 0.010*\"marta\" + 0.010*\"cafe\" + 0.010*\"visit\" + 0.010*\"get\" + 0.010*\"gracious\" + 0.010*\"welcomed\" + 0.010*\"return\" + 0.010*\"situated\"')\n",
      "(9, '0.041*\"staff\" + 0.041*\"space\" + 0.041*\"definitely\" + 0.041*\"need\" + 0.041*\"spot\" + 0.041*\"check\" + 0.041*\"current\" + 0.041*\"building\" + 0.041*\"coworking\" + 0.041*\"member\"')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load the reviews from the JSON file\n",
    "with open(os.path.join(os.getcwd(), \"TalentReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the text of the reviews\n",
    "reviews = [review.get(\"text\") for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "# Preprocess the reviews\n",
    "preprocessed_reviews = [preprocess(review) for review in reviews]\n",
    "\n",
    "# Create the dictionary\n",
    "dictionary = corpora.Dictionary(preprocessed_reviews)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
    "\n",
    "# Create the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the top 10 words for each topic\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word    weight\n",
      "0         day  0.010425\n",
      "1        cafe  0.010282\n",
      "2       visit  0.010251\n",
      "3    pleasant  0.010225\n",
      "4       marta  0.010141\n",
      "..        ...       ...\n",
      "95    walking  0.009881\n",
      "96   spacious  0.009881\n",
      "97     plenty  0.009881\n",
      "98  coworking  0.009881\n",
      "99       spot  0.009881\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract word weights from the LDA model\n",
    "word_weights = {dictionary[word_id]: weight for word_id, weight in lda_model.get_topic_terms(i, topn=len(dictionary))}\n",
    "\n",
    "# Convert word weights dictionary to a DataFrame\n",
    "df_talent = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_talent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LdaModel' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dictionary\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Plot word weights\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mplot_word_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mplot_word_weights\u001b[0;34m(lda_model, feature_names, n_top_words)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_word_weights\u001b[39m(lda_model, feature_names, n_top_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Combine probabilities of each word across all topics\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     word_weights \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents_\u001b[49m):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m topic\u001b[38;5;241m.\u001b[39margsort()[:\u001b[38;5;241m-\u001b[39mn_top_words \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m feature_names[j] \u001b[38;5;129;01min\u001b[39;00m word_weights:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LdaModel' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot word weights\n",
    "def plot_word_weights(lda_model, feature_names, n_top_words=10):\n",
    "    # Combine probabilities of each word across all topics\n",
    "    word_weights = {}\n",
    "    for i, topic in enumerate(lda_model.components_):\n",
    "        for j in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            if feature_names[j] in word_weights:\n",
    "                word_weights[feature_names[j]] += topic[j]\n",
    "            else:\n",
    "                word_weights[feature_names[j]] = topic[j]\n",
    "    \n",
    "    # Sort words by their weights\n",
    "    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    words, weights = zip(*sorted_word_weights)\n",
    "    \n",
    "    # Plot word weights\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(words)), weights, align='center', color='skyblue')\n",
    "    plt.yticks(range(len(words)), words, fontsize=10)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Word Weight')\n",
    "    plt.title('Talent Garden Madrid Word Weights')\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = list(dictionary.values())\n",
    "# Plot word weights\n",
    "plot_word_weights(lda_model, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weight for each word that was put in the review of the Talent Garden Madrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(review):\n",
    "    # Convert the review into lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Tokenize the review into individual words\n",
    "    words = word_tokenize(review)\n",
    "    \n",
    "    # Remove punctuation and stopwords\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(review) \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m preprocessed_reviews]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create the LDA model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLdaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Print the top 10 words for each topic\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m lda_model\u001b[38;5;241m.\u001b[39mprint_topics():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gensim/models/ldamodel.py:448\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_terms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_terms \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot compute LDA over an empty collection (no terms)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(distributed)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_topics)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load the reviews from the JSON file\n",
    "with open(os.path.join(os.getcwd(), \"FreelandReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the text of the reviews\n",
    "reviews = [review.get(\"text\") for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "# Preprocess the reviews\n",
    "preprocessed_reviews = [preprocess(review) for review in reviews]\n",
    "\n",
    "# Create the dictionary\n",
    "dictionary = corpora.Dictionary(preprocessed_reviews)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
    "\n",
    "# Create the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the top 10 words for each topic\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract word weights from the LDA model\n",
    "word_weights = {dictionary[word_id]: weight for word_id, weight in lda_model.get_topic_terms(i, topn=len(dictionary))}\n",
    "\n",
    "# Convert word weights dictionary to a DataFrame\n",
    "df_freeland = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_freeland)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot word weights for Gensim LdaModel\n",
    "def plot_word_weights_lda(lda_model, dictionary, n_top_words=10):\n",
    "    # Combine probabilities of each word across all topics\n",
    "    word_weights = {}\n",
    "    for i in range(lda_model.num_topics):\n",
    "        topic_words = lda_model.get_topic_terms(i, topn=n_top_words)\n",
    "        for word_id, weight in topic_words:\n",
    "            word = dictionary[word_id]\n",
    "            if word in word_weights:\n",
    "                word_weights[word] += weight\n",
    "            else:\n",
    "                word_weights[word] = weight\n",
    "    \n",
    "    # Sort words by their weights\n",
    "    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    words, weights = zip(*sorted_word_weights)\n",
    "    \n",
    "    # Plot word weights\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(words)), weights, align='center', color='skyblue')\n",
    "    plt.yticks(range(len(words)), words, fontsize=8)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Word Weight')\n",
    "    plt.title('')\n",
    "    plt.show()\n",
    "\n",
    "# Plot word weights\n",
    "plot_word_weights_lda(lda_model, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weight for each word that was put in the review of the Freeland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge the two DataFrames on the 'word' column\n",
    "merged_df = df_freeland.merge(df_talent, on='word', how='inner')\n",
    "\n",
    "# Rename the columns\n",
    "merged_df.columns = ['word', 'weight_freeland', 'weight_talent']\n",
    "\n",
    "# Create a new column for the differences between the weights\n",
    "merged_df['weight_difference'] = abs(merged_df['weight_freeland'] - merged_df['weight_talent'])\n",
    "\n",
    "# Sort the DataFrame by the 'weight_difference' column in descending order\n",
    "merged_df = merged_df.sort_values(by='weight_difference', ascending=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The common words that was used in the reviews for each place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "merged_df.to_csv('merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can see that the words: accessible, spacious, different, people and confortable have more impact for those two best working spaces in Madrid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
