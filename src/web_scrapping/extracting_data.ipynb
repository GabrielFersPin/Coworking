{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the best coworking space (Talent Garden Madrid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the GOOGLE_PLACES_API_KEY environment variable\n",
    "os.environ[\"GOOGLE_PLACES_API_KEY\"] = \"AIzaSyD_NUIC2KPfD-8euMjdKgpBsLB05MFzSgE\"\n",
    "\n",
    "def fetch_reviews(place_id, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\n",
    "        'place_id': place_id,\n",
    "        \"X-Goog-FieldMask\": \"places.reviews\",\n",
    "        'key': api_key,\n",
    "    }\n",
    "    print(f\"Making API request to URL: {url} with query parameters: {params}\")\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception if the status code indicates an error occurred\n",
    "        data = response.json()\n",
    "        return data.get('result', {}).get('reviews', [])\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        print(\"Please check your API key and usage limits.\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error occurred: {err}\")\n",
    "    return []\n",
    "\n",
    "place_id = \"ChIJrTH0vwYnQg0RwFJPB0NzKeE\"  # Example place ID\n",
    "api_key = os.environ[\"GOOGLE_PLACES_API_KEY\"]\n",
    "reviews = fetch_reviews(place_id, api_key)\n",
    "\n",
    "if reviews:\n",
    "    print(\"Reviews fetched successfully:\")\n",
    "    for review in reviews:\n",
    "        print(f\"Author: {review.get('author_name')}, Rating: {review.get('rating')}, Review: {review.get('text')}\")\n",
    "else:\n",
    "    print(\"Error: Unable to fetch reviews from the API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews for Talent Garden Madrid:\n",
      "The most flexible and conveniently located office in Madrid. The ergonomic seat and large display made me feel better than in a corporate office. Will repeat.\n",
      "I want to say a huge thank you to the gentleman at the reception today. I had a very bad day, and he helped me and make this bad day a better day. Thank you, this world needs more people kind like you. ðŸ™ðŸ½\n",
      "The best co-working I have worked in yet. For a very reasonable price you can rent a very spacious desk and individual space that strike a great balance between privacy and interaction with fellow co-workers. The interior is very well taken care off with enough attention to detail to make you feel comfortable, yet without the frills that would only cause distraction.\n",
      "\n",
      "Really recommended is the personal approach of the owner Pancho and his team, always showing genuine interest in your ideas and projects and always there to help you out with whatever you need.\n",
      "I really enjoyed my time working at Freeland and will be sure to return when Im back in Madrid full time next year. Nina and the gang took amazing care of me and went always above and beyond. The spacious, fresh work space is a lovely place to come in the morning and its located in a very accessible area.\n",
      "Tired from working at home for years, Freeland was the answer to my troubles. Comfort, collaborations, social life, and productivity would be my four words of choice to describe working here.\n",
      "\n",
      "There is a great mix of businesses and freelancers from different job sectors and countries. The installations are lighthearted and functional, inspiring productivity without distractions.\n",
      "\n",
      "The owner Francisco is constantly working to help encourage new relationships between businesses and people.\n",
      "\n",
      "I would definitely recommend Freeland to anyone looking for a workspace where they can take their business and interpersonal life to the next level.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load the data from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"/workspaces/Coworking/src/results/FreelandReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print the reviews for the place\n",
    "print(\"Reviews for Talent Garden Madrid:\")\n",
    "\n",
    "for review in data.get(\"result\", {}).get(\"reviews\", []):\n",
    "    print(review.get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the VADER sentiment analysis model from NLTK\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Define the sentiment analysis function\n",
    "def analyze_sentiment(text):\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    if sentiment['compound'] > 0.05:\n",
    "        return 'positive'\n",
    "    elif sentiment['compound'] < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Load the reviews from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"/workspaces/Coworking/src/results/FreelandReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Classify the reviews as positive, negative, or neutral\n",
    "for review in data.get(\"result\", {}).get(\"reviews\", []):\n",
    "    sentiment = analyze_sentiment(review.get(\"text\"))\n",
    "    print(f\"Review sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at /workspaces/Coworking/src/web_scrapping/workspaces/Coworking/src/results/FreelandReviews.json\n",
      "Most common words in the reviews:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "import nltk\n",
    "\n",
    "# Ensure resources are available\n",
    "for resource in ['punkt', 'stopwords', 'wordnet']:\n",
    "    try:\n",
    "        nltk.data.find(f'corpora/{resource}' if resource != 'punkt' else f'tokenizers/{resource}')\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        words = word_tokenize(text.lower())\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "        words = word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(stemmer.stem(word) for word in words)\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(os.getcwd(), \"workspaces\", \"Coworking\", \"src\", \"results\", \"FreelandReviews.json\")\n",
    "\n",
    "# Load the reviews\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    data = {\"result\": {\"reviews\": []}}\n",
    "else:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "# Process reviews\n",
    "reviews = [preprocess(review.get(\"text\", \"\")) for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = collections.Counter(word for review in reviews for word in review.split())\n",
    "\n",
    "# Display results\n",
    "print(\"Most common words in the reviews:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "are the by to definitely staff this as spaces very\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "nice by to clean some centre modern near room expansion\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "nice light close atocha day to by bars plenty expansion\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "the and was to for me day on of very\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "noisy but useless phone ceiling open booths cold no walls\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_ for token in doc]\n",
    "    cleaned_tokens = [token for token in tokens if token and token not in stop_words and token.isalpha()]\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Load the reviews from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"TalentReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the text of the reviews\n",
    "reviews = [review.get(\"text\") for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "\n",
    "# Skip preprocessing if the data is already preprocessed\n",
    "if all(isinstance(review, str) for review in reviews):\n",
    "    cleaned_reviews = reviews\n",
    "else:\n",
    "    cleaned_reviews = [preprocess(review) for review in reviews]\n",
    "\n",
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the cleaned_reviews into a matrix of token counts\n",
    "review_matrix = vectorizer.fit_transform(cleaned_reviews)\n",
    "\n",
    "# Get the list of feature names after fitting the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a LatentDirichletAllocation model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "# Fit the model to the review_matrix\n",
    "lda.fit(review_matrix)\n",
    "\n",
    "# Display the topics\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {i}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[-10:][::-1]]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(review):\n",
    "    # Convert the review into lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Tokenize the review into individual words\n",
    "    words = word_tokenize(review)\n",
    "    \n",
    "    # Remove punctuation and stopwords\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"nice\" + 0.010*\"day\" + 0.010*\"atocha\" + 0.010*\"light\" + 0.010*\"close\" + 0.010*\"space\" + 0.010*\"booth\" + 0.010*\"useless\" + 0.010*\"wall\" + 0.010*\"ceiling\"')\n",
      "(1, '0.046*\"noisy\" + 0.046*\"ceiling\" + 0.046*\"booth\" + 0.046*\"cold\" + 0.046*\"phone\" + 0.046*\"wall\" + 0.046*\"open\" + 0.046*\"useless\" + 0.046*\"space\" + 0.046*\"light\"')\n",
      "(2, '0.010*\"day\" + 0.010*\"visit\" + 0.010*\"cafe\" + 0.010*\"room\" + 0.010*\"space\" + 0.010*\"pleasant\" + 0.010*\"marta\" + 0.010*\"noisy\" + 0.010*\"walked\" + 0.010*\"close\"')\n",
      "(3, '0.010*\"day\" + 0.010*\"light\" + 0.010*\"atocha\" + 0.010*\"nice\" + 0.010*\"space\" + 0.010*\"noisy\" + 0.010*\"close\" + 0.010*\"booth\" + 0.010*\"useless\" + 0.010*\"open\"')\n",
      "(4, '0.010*\"day\" + 0.010*\"cafe\" + 0.010*\"visit\" + 0.010*\"pleasant\" + 0.010*\"marta\" + 0.010*\"way\" + 0.010*\"purchase\" + 0.010*\"pas\" + 0.010*\"thank\" + 0.010*\"first\"')\n",
      "(5, '0.010*\"nice\" + 0.010*\"day\" + 0.010*\"close\" + 0.010*\"atocha\" + 0.010*\"light\" + 0.010*\"space\" + 0.010*\"booth\" + 0.010*\"madrid\" + 0.010*\"open\" + 0.010*\"noisy\"')\n",
      "(6, '0.083*\"nice\" + 0.044*\"room\" + 0.044*\"office\" + 0.044*\"madrid\" + 0.044*\"distance\" + 0.044*\"clean\" + 0.044*\"centre\" + 0.044*\"bar\" + 0.044*\"modern\" + 0.044*\"expansion\"')\n",
      "(7, '0.038*\"day\" + 0.026*\"marta\" + 0.026*\"visit\" + 0.026*\"pleasant\" + 0.026*\"cafe\" + 0.014*\"walked\" + 0.014*\"door\" + 0.014*\"trip\" + 0.014*\"gracious\" + 0.014*\"selection\"')\n",
      "(8, '0.010*\"day\" + 0.010*\"pleasant\" + 0.010*\"marta\" + 0.010*\"cafe\" + 0.010*\"visit\" + 0.010*\"get\" + 0.010*\"gracious\" + 0.010*\"welcomed\" + 0.010*\"return\" + 0.010*\"situated\"')\n",
      "(9, '0.041*\"staff\" + 0.041*\"space\" + 0.041*\"definitely\" + 0.041*\"need\" + 0.041*\"spot\" + 0.041*\"check\" + 0.041*\"current\" + 0.041*\"building\" + 0.041*\"coworking\" + 0.041*\"member\"')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load the reviews from the JSON file\n",
    "with open(os.path.join(os.getcwd(), \"TalentReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the text of the reviews\n",
    "reviews = [review.get(\"text\") for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "# Preprocess the reviews\n",
    "preprocessed_reviews = [preprocess(review) for review in reviews]\n",
    "\n",
    "# Create the dictionary\n",
    "dictionary = corpora.Dictionary(preprocessed_reviews)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
    "\n",
    "# Create the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the top 10 words for each topic\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word    weight\n",
      "0         day  0.010425\n",
      "1        cafe  0.010282\n",
      "2       visit  0.010251\n",
      "3    pleasant  0.010225\n",
      "4       marta  0.010141\n",
      "..        ...       ...\n",
      "95    walking  0.009881\n",
      "96   spacious  0.009881\n",
      "97     plenty  0.009881\n",
      "98  coworking  0.009881\n",
      "99       spot  0.009881\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract word weights from the LDA model\n",
    "word_weights = {dictionary[word_id]: weight for word_id, weight in lda_model.get_topic_terms(i, topn=len(dictionary))}\n",
    "\n",
    "# Convert word weights dictionary to a DataFrame\n",
    "df_talent = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_talent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Get feature names\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdictionary\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Plot word weights\u001b[39;00m\n\u001b[1;32m     30\u001b[0m plot_word_weights(lda_model, feature_names)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot word weights\n",
    "def plot_word_weights(lda_model, feature_names, n_top_words=10):\n",
    "    # Combine probabilities of each word across all topics\n",
    "    word_weights = {}\n",
    "    for i, topic in enumerate(lda_model.components_):\n",
    "        for j in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            if feature_names[j] in word_weights:\n",
    "                word_weights[feature_names[j]] += topic[j]\n",
    "            else:\n",
    "                word_weights[feature_names[j]] = topic[j]\n",
    "    \n",
    "    # Sort words by their weights\n",
    "    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    words, weights = zip(*sorted_word_weights)\n",
    "    \n",
    "    # Plot word weights\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(words)), weights, align='center', color='skyblue')\n",
    "    plt.yticks(range(len(words)), words, fontsize=10)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Word Weight')\n",
    "    plt.title('Talent Garden Madrid Word Weights')\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = list(dictionary.values())\n",
    "# Plot word weights\n",
    "plot_word_weights(lda_model, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the second best Coworking Space (Freeland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API request to URL: https://maps.googleapis.com/maps/api/place/details/json with query parameters: {'place_id': 'ChIJCVUBCA4mQg0RbIccVWP9JA8', 'key': 'AIzaSyD_NUIC2KPfD-8euMjdKgpBsLB05MFzSgE'} and headers: {'X-Goog-FieldMask': 'places.reviews'}\n",
      "Data saved as FreelandReviews.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the GOOGLE_PLACES_API_KEY environment variable\n",
    "os.environ[\"GOOGLE_PLACES_API_KEY\"] = \"AIzaSyD_NUIC2KPfD-8euMjdKgpBsLB05MFzSgE\"\n",
    "\n",
    "def fetch_reviews(place_id, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    headers = {\n",
    "        \"X-Goog-FieldMask\": \"places.reviews\"\n",
    "    }\n",
    "    params = {\n",
    "        'place_id': place_id,\n",
    "        'key': api_key,\n",
    "    }\n",
    "    print(f\"Making API request to URL: {url} with query parameters: {params} and headers: {headers}\")\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception if the status code indicates an error occurred\n",
    "        data = response.json()\n",
    "        return data\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        print(\"Please check your API key and usage limits.\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error occurred: {err}\")\n",
    "    return None\n",
    "\n",
    "place_id = \"ChIJCVUBCA4mQg0RbIccVWP9JA8\"\n",
    "api_key = os.environ[\"GOOGLE_PLACES_API_KEY\"]\n",
    "data = fetch_reviews(place_id, api_key)\n",
    "\n",
    "if data:\n",
    "    # Save the data as a JSON file\n",
    "    with open(os.path.join(os.getcwd(), \"FreelandReviews.json\"), \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(\"Data saved as FreelandReviews.json\")\n",
    "else:\n",
    "    print(\"Error: Unable to fetch reviews from the API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews for Freeland:\n",
      "The most flexible and conveniently located office in Madrid. The ergonomic seat and large display made me feel better than in a corporate office. Will repeat.\n",
      "I want to say a huge thank you to the gentleman at the reception today. I had a very bad day, and he helped me and make this bad day a better day. Thank you, this world needs more people kind like you. ðŸ™ðŸ½\n",
      "The best co-working I have worked in yet. For a very reasonable price you can rent a very spacious desk and individual space that strike a great balance between privacy and interaction with fellow co-workers. The interior is very well taken care off with enough attention to detail to make you feel comfortable, yet without the frills that would only cause distraction.\n",
      "\n",
      "Really recommended is the personal approach of the owner Pancho and his team, always showing genuine interest in your ideas and projects and always there to help you out with whatever you need.\n",
      "I really enjoyed my time working at Freeland and will be sure to return when Im back in Madrid full time next year. Nina and the gang took amazing care of me and went always above and beyond. The spacious, fresh work space is a lovely place to come in the morning and its located in a very accessible area.\n",
      "Tired from working at home for years, Freeland was the answer to my troubles. Comfort, collaborations, social life, and productivity would be my four words of choice to describe working here.\n",
      "\n",
      "There is a great mix of businesses and freelancers from different job sectors and countries. The installations are lighthearted and functional, inspiring productivity without distractions.\n",
      "\n",
      "The owner Francisco is constantly working to help encourage new relationships between businesses and people.\n",
      "\n",
      "I would definitely recommend Freeland to anyone looking for a workspace where they can take their business and interpersonal life to the next level.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load the data from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"/workspaces/Coworking/src/web_scrapping/FreelandReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print the reviews for the place\n",
    "print(\"Reviews for Freeland:\")\n",
    "\n",
    "for review in data.get(\"result\", {}).get(\"reviews\", []):\n",
    "    print(review.get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n",
      "Review sentiment: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the VADER sentiment analysis model from NLTK\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Define the sentiment analysis function\n",
    "def analyze_sentiment(text):\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    if sentiment['compound'] > 0.05:\n",
    "        return 'positive'\n",
    "    elif sentiment['compound'] < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Load the reviews from the \"TalentReviews.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"/workspaces/Coworking/src/web_scrapping/FreelandReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Classify the reviews as positive, negative, or neutral\n",
    "for review in data.get(\"result\", {}).get(\"reviews\", []):\n",
    "    sentiment = analyze_sentiment(review.get(\"text\"))\n",
    "    print(f\"Review sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/workspaces/Coworking/.venv/nltk_data'\n    - '/workspaces/Coworking/.venv/share/nltk_data'\n    - '/workspaces/Coworking/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Extract the text of the reviews\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m reviews \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Count the frequency of each word in the preprocessed reviews\u001b[39;00m\n\u001b[1;32m     31\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter(word \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m reviews \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review\u001b[38;5;241m.\u001b[39msplit())\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m     18\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 19\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha()]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words)\n",
      "File \u001b[0;32m/workspaces/Coworking/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/workspaces/Coworking/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/workspaces/Coworking/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/Coworking/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/Coworking/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/workspaces/Coworking/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/workspaces/Coworking/.venv/nltk_data'\n    - '/workspaces/Coworking/.venv/share/nltk_data'\n    - '/workspaces/Coworking/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk.corpus\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Download the NLTK English tokenizer and stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(stemmer.stem(word) for word in words)\n",
    "\n",
    "# Load the reviews from the \"Freeland.json\" file\n",
    "with open(os.path.join(os.getcwd(), \"/workspaces/Coworking/src/results/FreelandReviews.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the text of the reviews\n",
    "reviews = [preprocess(review.get(\"text\")) for review in data.get(\"result\", {}).get(\"reviews\", [])]\n",
    "\n",
    "# Count the frequency of each word in the preprocessed reviews\n",
    "word_counts = collections.Counter(word for review in reviews for word in review.split())\n",
    "\n",
    "# Print the most common words in the reviews\n",
    "print(\"Most common words in the reviews:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(review):\n",
    "    # Convert the review into lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Tokenize the review into individual words\n",
    "    words = word_tokenize(review)\n",
    "    \n",
    "    # Remove punctuation and stopwords\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Get feature names\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdictionary\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Plot word weights\u001b[39;00m\n\u001b[1;32m     30\u001b[0m plot_word_weights(lda_model, feature_names)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot word weights\n",
    "def plot_word_weights(lda_model, feature_names, n_top_words=10):\n",
    "    # Combine probabilities of each word across all topics\n",
    "    word_weights = {}\n",
    "    for i, topic in enumerate(lda_model.components_):\n",
    "        for j in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            if feature_names[j] in word_weights:\n",
    "                word_weights[feature_names[j]] += topic[j]\n",
    "            else:\n",
    "                word_weights[feature_names[j]] = topic[j]\n",
    "    \n",
    "    # Sort words by their weights\n",
    "    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    words, weights = zip(*sorted_word_weights)\n",
    "    \n",
    "    # Plot word weights\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(words)), weights, align='center', color='skyblue')\n",
    "    plt.yticks(range(len(words)), words, fontsize=10)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Word Weight')\n",
    "    plt.title('Talent Garden Madrid Word Weights')\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = list(dictionary.values())\n",
    "# Plot word weights\n",
    "plot_word_weights(lda_model, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weight for each word that was put in the review of the Talent Garden Madrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(review):\n",
    "    # Convert the review into lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Tokenize the review into individual words\n",
    "    words = word_tokenize(review)\n",
    "    \n",
    "    # Remove punctuation and stopwords\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract word weights from the LDA model\n",
    "word_weights = {dictionary[word_id]: weight for word_id, weight in lda_model.get_topic_terms(i, topn=len(dictionary))}\n",
    "\n",
    "# Convert word weights dictionary to a DataFrame\n",
    "df_freeland = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_freeland)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot word weights for Gensim LdaModel\n",
    "def plot_word_weights_lda(lda_model, dictionary, n_top_words=10):\n",
    "    # Combine probabilities of each word across all topics\n",
    "    word_weights = {}\n",
    "    for i in range(lda_model.num_topics):\n",
    "        topic_words = lda_model.get_topic_terms(i, topn=n_top_words)\n",
    "        for word_id, weight in topic_words:\n",
    "            word = dictionary[word_id]\n",
    "            if word in word_weights:\n",
    "                word_weights[word] += weight\n",
    "            else:\n",
    "                word_weights[word] = weight\n",
    "    \n",
    "    # Sort words by their weights\n",
    "    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    words, weights = zip(*sorted_word_weights)\n",
    "    \n",
    "    # Plot word weights\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(words)), weights, align='center', color='skyblue')\n",
    "    plt.yticks(range(len(words)), words, fontsize=8)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Word Weight')\n",
    "    plt.title('')\n",
    "    plt.show()\n",
    "\n",
    "# Plot word weights\n",
    "plot_word_weights_lda(lda_model, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weight for each word that was put in the review of the Freeland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge the two DataFrames on the 'word' column\n",
    "merged_df = df_freeland.merge(df_talent, on='word', how='inner')\n",
    "\n",
    "# Rename the columns\n",
    "merged_df.columns = ['word', 'weight_freeland', 'weight_talent']\n",
    "\n",
    "# Create a new column for the differences between the weights\n",
    "merged_df['weight_difference'] = abs(merged_df['weight_freeland'] - merged_df['weight_talent'])\n",
    "\n",
    "# Sort the DataFrame by the 'weight_difference' column in descending order\n",
    "merged_df = merged_df.sort_values(by='weight_difference', ascending=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The common words that was used in the reviews for each place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "merged_df.to_csv('merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can see that the words: accessible, spacious, different, people and confortable have more impact for those two best working spaces in Madrid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
