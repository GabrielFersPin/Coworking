{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap from coworker.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening URL: https://www.coworker.com/spain/madrid?view=list\n",
      "Waiting for page to load...\n",
      "Checking if main content is accessible...\n",
      "Found 1 <main> elements\n",
      "Found 10 carousel tracks\n",
      "\n",
      "Extracting coworking space links...\n",
      "Found 10 unique coworking space links\n",
      "\n",
      "Extracting information for each coworking space...\n",
      "Processing link 1/10: https://www.coworker.com/spain/madrid/regus-madrid-la-moraleja\n",
      "  - Name: Coworking Space: Regus - Madrid, La Moraleja in Madrid\n",
      "Processing link 2/10: https://www.coworker.com/spain/madrid/regus-madrid-torre-de-cristal\n",
      "  - Name: Coworking Space: Regus - Madrid, Torre de Cristal in Madrid\n",
      "Processing link 3/10: https://www.coworker.com/spain/madrid/regus-madrid-ortega-y-gasset\n",
      "  - Name: Coworking Space: Regus - Madrid, Ortega y Gasset in Madrid\n",
      "Processing link 4/10: https://www.coworker.com/spain/madrid/regus-madrid-manoteras\n",
      "  - Name: Coworking Space: Regus - Madrid, Manoteras in Madrid\n",
      "Processing link 5/10: https://www.coworker.com/spain/madrid/regus-madrid-pinar-salamanca-district\n",
      "  - Name: Coworking Space: Regus - Madrid Pinar-Salamanca District in Madrid\n",
      "Processing link 6/10: https://www.coworker.com/spain/madrid/regus-las-rozas-las-rozas\n",
      "  - Name: Coworking Space: Regus - LAS ROZAS, Las Rozas in Madrid\n",
      "Processing link 7/10: https://www.coworker.com/spain/madrid/regus-madrid-avenida-america\n",
      "  - Name: Coworking Space: Regus - Madrid, Avenida America in Madrid\n",
      "Processing link 8/10: https://www.coworker.com/spain/madrid/wework-eloy-gonzalo-27\n",
      "  - Name: Coworking Space: WeWork Eloy Gonzalo 27 in Madrid\n",
      "Processing link 9/10: https://www.coworker.com/spain/madrid/regus-madrid-colon\n",
      "  - Name: Coworking Space: Regus - Madrid, Colon in Madrid\n",
      "Processing link 10/10: https://www.coworker.com/spain/madrid/regus-madrid-financial-district-torre-europa\n",
      "  - Name: Coworking Space: Regus - Madrid Financial District - Torre Europa in Madrid\n",
      "\n",
      "Saving data for 10 coworking spaces...\n",
      "Data saved successfully!\n",
      "\n",
      "Attempting to extract data directly from list page...\n",
      "Found 10 potential list items with selector: div[class*='card']\n",
      "Extracting data from 10 list items...\n",
      "  Item 1: Regus - Madrid, La Moraleja in Madrid\n",
      "  Item 2: Regus - Madrid, Torre de Cristal in Madrid\n",
      "  Item 3: Regus - Madrid, Ortega y Gasset in Madrid\n",
      "  Item 4: Regus - Madrid, Manoteras in Madrid\n",
      "  Item 5: Regus - Madrid Pinar-Salamanca District in Madrid\n",
      "  Item 6: Regus - LAS ROZAS, Las Rozas in Madrid\n",
      "  Item 7: Regus - Madrid, Avenida America in Madrid\n",
      "  Item 8: WeWork Eloy Gonzalo 27 in Madrid\n",
      "  Item 9: Regus - Madrid, Colon in Madrid\n",
      "  Item 10: Regus - Madrid Financial District - Torre Europa in Madrid\n",
      "List data saved successfully!\n",
      "\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "chromedriver_path = \"/workspaces/Coworking/chromedriver-linux64/chromedriver\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless=false\")  # Run with visible browser\n",
    "\n",
    "# Anti-detection measures\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Set user agent to appear more like a regular browser\n",
    "driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
    "    \"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "})\n",
    "\n",
    "try:\n",
    "    # Open coworking page\n",
    "    city = \"madrid\"\n",
    "    base_url = f\"https://www.coworker.com/spain/{city}?view=list\"\n",
    "    print(f\"Opening URL: {base_url}\")\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    print(\"Waiting for page to load...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # First, check if we're getting the main content\n",
    "    print(\"Checking if main content is accessible...\")\n",
    "    try:\n",
    "        # Look for main sections\n",
    "        main_elements = driver.find_elements(By.TAG_NAME, \"main\")\n",
    "        if main_elements:\n",
    "            print(f\"Found {len(main_elements)} <main> elements\")\n",
    "            \n",
    "        # Look for the slick carousel we identified\n",
    "        carousel_elements = driver.find_elements(By.CSS_SELECTOR, \".slick-track\")\n",
    "        if carousel_elements:\n",
    "            print(f\"Found {len(carousel_elements)} carousel tracks\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding main elements: {e}\")\n",
    "    \n",
    "    # Now let's extract the coworking space links directly\n",
    "    print(\"\\nExtracting coworking space links...\")\n",
    "    coworking_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/spain/madrid/']\")\n",
    "    filtered_links = []\n",
    "    \n",
    "    for link in coworking_links:\n",
    "        href = link.get_attribute(\"href\")\n",
    "        if href and \"/search?\" not in href and \"/explore?\" not in href:\n",
    "            # This is likely a link to a specific coworking space\n",
    "            filtered_links.append({\n",
    "                \"href\": href,\n",
    "                \"text\": link.text.strip()\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates based on href\n",
    "    unique_links = []\n",
    "    seen_hrefs = set()\n",
    "    for link in filtered_links:\n",
    "        if link[\"href\"] not in seen_hrefs:\n",
    "            seen_hrefs.add(link[\"href\"])\n",
    "            unique_links.append(link)\n",
    "    \n",
    "    print(f\"Found {len(unique_links)} unique coworking space links\")\n",
    "    \n",
    "    # Extract information for each coworking space\n",
    "    coworking_spaces = []\n",
    "    \n",
    "    print(\"\\nExtracting information for each coworking space...\")\n",
    "    for i, link_data in enumerate(unique_links):\n",
    "        try:\n",
    "            if i < 10:  # Process first 10 links for demonstration\n",
    "                print(f\"Processing link {i+1}/{len(unique_links)}: {link_data['href']}\")\n",
    "                \n",
    "                # Open the coworking space page\n",
    "                driver.get(link_data[\"href\"])\n",
    "                time.sleep(3)  # Wait for page to load\n",
    "                \n",
    "                # Extract data\n",
    "                space_data = {\n",
    "                    \"url\": link_data[\"href\"],\n",
    "                    \"name\": \"\",\n",
    "                    \"address\": \"\",\n",
    "                    \"description\": \"\",\n",
    "                    \"amenities\": []\n",
    "                }\n",
    "                \n",
    "                # Name (try different selectors)\n",
    "                try:\n",
    "                    name_element = driver.find_element(By.CSS_SELECTOR, \"h1\")\n",
    "                    space_data[\"name\"] = name_element.text.strip()\n",
    "                except:\n",
    "                    try:\n",
    "                        name_element = driver.find_element(By.CSS_SELECTOR, \"[class*='title']\")\n",
    "                        space_data[\"name\"] = name_element.text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Address\n",
    "                try:\n",
    "                    address_element = driver.find_element(By.CSS_SELECTOR, \"[class*='address'], [class*='location']\")\n",
    "                    space_data[\"address\"] = address_element.text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Description\n",
    "                try:\n",
    "                    desc_element = driver.find_element(By.CSS_SELECTOR, \"[class*='description'], [class*='about']\")\n",
    "                    space_data[\"description\"] = desc_element.text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Amenities\n",
    "                try:\n",
    "                    amenity_elements = driver.find_elements(By.CSS_SELECTOR, \"[class*='amenity'], [class*='facility']\")\n",
    "                    space_data[\"amenities\"] = [el.text.strip() for el in amenity_elements if el.text.strip()]\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                coworking_spaces.append(space_data)\n",
    "                print(f\"  - Name: {space_data['name']}\")\n",
    "                \n",
    "                # Go back to the list page if we need to process more spaces\n",
    "                if i < len(unique_links) - 1:\n",
    "                    driver.back()\n",
    "                    time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing link {i+1}: {e}\")\n",
    "    \n",
    "    # Save the extracted data\n",
    "    print(f\"\\nSaving data for {len(coworking_spaces)} coworking spaces...\")\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(\"/workspaces/Coworking/coworking_spaces.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(coworking_spaces, f, indent=2)\n",
    "    \n",
    "    # Save as CSV\n",
    "    with open(\"/workspaces/Coworking/coworking_spaces.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        fieldnames = [\"name\", \"address\", \"url\", \"description\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for space in coworking_spaces:\n",
    "            # Create a new dict with just the fields we want for CSV\n",
    "            csv_row = {field: space.get(field, \"\") for field in fieldnames}\n",
    "            writer.writerow(csv_row)\n",
    "    \n",
    "    print(\"Data saved successfully!\")\n",
    "    \n",
    "    # Alternative approach: Try to extract directly from the list page\n",
    "    print(\"\\nAttempting to extract data directly from list page...\")\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # The links are within a slider, but we also need to try finding standard list items\n",
    "    list_selectors = [\n",
    "        \"div.slick-track > div\",  # The slider items we identified\n",
    "        \"div[class*='card']\",     # Common card pattern\n",
    "        \"div[class*='listing']\",  # Common listing pattern\n",
    "        \"div[class*='result']\",   # Search result pattern\n",
    "        \"main > div > div\"        # Generic nested divs that might contain listings\n",
    "    ]\n",
    "    \n",
    "    list_items = []\n",
    "    working_selector = \"\"\n",
    "    \n",
    "    for selector in list_selectors:\n",
    "        try:\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if elements and len(elements) > 5 and len(elements) < 100:\n",
    "                print(f\"Found {len(elements)} potential list items with selector: {selector}\")\n",
    "                list_items = elements\n",
    "                working_selector = selector\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if list_items:\n",
    "        print(f\"Extracting data from {len(list_items)} list items...\")\n",
    "        list_data = []\n",
    "        \n",
    "        for i, item in enumerate(list_items[:10]):  # Process first 10 items\n",
    "            try:\n",
    "                item_data = {\"position\": i + 1}\n",
    "                \n",
    "                # Try to get the link\n",
    "                links = item.find_elements(By.TAG_NAME, \"a\")\n",
    "                if links:\n",
    "                    item_data[\"url\"] = links[0].get_attribute(\"href\")\n",
    "                \n",
    "                # Try to get name/title\n",
    "                try:\n",
    "                    title_els = item.find_elements(By.CSS_SELECTOR, \"h1, h2, h3, h4, strong, [class*='title'], [class*='name']\")\n",
    "                    if title_els:\n",
    "                        item_data[\"name\"] = title_els[0].text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try to get address\n",
    "                try:\n",
    "                    addr_els = item.find_elements(By.CSS_SELECTOR, \"[class*='address'], [class*='location']\")\n",
    "                    if addr_els:\n",
    "                        item_data[\"address\"] = addr_els[0].text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try to get image URL\n",
    "                try:\n",
    "                    img_els = item.find_elements(By.TAG_NAME, \"img\")\n",
    "                    if img_els:\n",
    "                        item_data[\"image_url\"] = img_els[0].get_attribute(\"src\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                list_data.append(item_data)\n",
    "                print(f\"  Item {i+1}: {item_data.get('name', 'Unknown')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing list item {i+1}: {e}\")\n",
    "        \n",
    "        # Save list data\n",
    "        with open(\"/workspaces/Coworking/list_items.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(list_data, f, indent=2)\n",
    "        \n",
    "        print(\"List data saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during extraction: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Clean up\n",
    "    driver.quit()\n",
    "    print(\"\\nExtraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting web scraping...\n",
      "Page title: Just a moment...\n",
      "âŒ Error finding coworking blocks: Message: \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Coworking Name, Address, Amenities, Price (per month)]\n",
      "Index: [] Total rows: 0\n",
      "âœ… CSV file saved at /workspaces/Coworking/src/results/Madrid/coworkings_madrid.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def scrape_coworker_data(driver, city=\"madrid\"):\n",
    "    base_url = f\"https://www.coworker.com/spain/{city}?view=list\"\n",
    "    driver.get(base_url)\n",
    "\n",
    "    time.sleep(5)  # Wait for content to load\n",
    "\n",
    "    # âœ… Save page source for debugging\n",
    "    with open(f\"/workspaces/Coworking/page_source_{city}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(driver.page_source)\n",
    "\n",
    "    # âœ… Print page title to confirm loading\n",
    "    print(f\"Page title: {driver.title}\")\n",
    "\n",
    "    coworkings, addresses, amenities, prices = [], [], [], []\n",
    "\n",
    "    while True:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        try:\n",
    "            coworking_blocks = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.SearchResult_container__0EI6G\")))\n",
    "            print(f\"Found {len(coworking_blocks)} coworking spaces on this page.\")\n",
    "        except Exception as e:\n",
    "            print(\"âŒ Error finding coworking blocks:\", e)\n",
    "            break\n",
    "\n",
    "        for block in coworking_blocks:\n",
    "            try:\n",
    "                name = block.find_element(By.CSS_SELECTOR, \"p.SearchResult_container__content__title__fSW47\").text\n",
    "                address = block.find_element(By.CSS_SELECTOR, \"span.Transportation_name__CkTxv\").text\n",
    "                amenities_str = \", \".join([a.text for a in block.find_elements(By.CSS_SELECTOR, \"div.amenities li\")]) or \"N/A\"\n",
    "                price = block.find_element(By.CSS_SELECTOR, \"div.Prices_container__-8jBv p\").text or \"N/A\"\n",
    "\n",
    "                coworkings.append(name)\n",
    "                addresses.append(address)\n",
    "                amenities.append(amenities_str)\n",
    "                prices.append(price)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error scraping a coworking space: {e}\")\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button.Pagination_page_link__4IZxn\")\n",
    "            next_button.click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print(\"âœ… No more pages. Scraping complete.\")\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Coworking Name\": coworkings,\n",
    "        \"Address\": addresses,\n",
    "        \"Amenities\": amenities,\n",
    "        \"Price (per month)\": prices\n",
    "    })\n",
    "\n",
    "    print(df.head(), f\"Total rows: {df.shape[0]}\")\n",
    "\n",
    "    save_path = \"/workspaces/Coworking/src/results/Madrid/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    csv_filename = os.path.join(save_path, f\"coworkings_{city}.csv\")\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"âœ… CSV file saved at {csv_filename}\")\n",
    "\n",
    "# ---------- Main Execution ----------\n",
    "if __name__ == \"__main__\":\n",
    "    chromedriver_path = \"/workspaces/Coworking/chromedriver-linux64/chromedriver\"\n",
    "\n",
    "    if not os.path.exists(chromedriver_path):\n",
    "        raise FileNotFoundError(f\"ChromeDriver not found at {chromedriver_path}\")\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--headless=false\")  # âœ… Run with a visible browser for debugging\n",
    "\n",
    "    service = Service(chromedriver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    print(\"ðŸš€ Starting web scraping...\")\n",
    "    scrape_coworker_data(driver, city=\"madrid\")\n",
    "\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
