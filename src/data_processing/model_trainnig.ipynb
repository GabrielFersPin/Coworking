{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coworking Space Clustering Model Extraction\n",
    "\n",
    "This notebook extracts the clustering model from the Streamlit app, trains it on the data, and saves both the model and cleaned data for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions from Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse string representations of lists into actual lists\n",
    "def parse_amenities(amenity_str):\n",
    "    if pd.isna(amenity_str) or amenity_str == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(amenity_str)\n",
    "    except:\n",
    "        try:\n",
    "            return ast.literal_eval(amenity_str.replace('\"', ''))\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "# Function to find amenities in description text\n",
    "def find_amenities_in_description(description, amenities_list):\n",
    "    if pd.isna(description):\n",
    "        return []\n",
    "    description = description.lower()\n",
    "    found_amenities = []\n",
    "    amenity_keywords = {\n",
    "        'wifi': 'wifi',\n",
    "        'internet': 'wifi',\n",
    "        'coffee': 'coffee',\n",
    "        'café': 'coffee',\n",
    "        'cafe': 'coffee',\n",
    "        'kitchen': 'kitchen',\n",
    "        'parking': 'parking',\n",
    "        'bike': 'bike_storage',\n",
    "        'bicycle': 'bike_storage',\n",
    "        'locker': 'locker',\n",
    "        'meeting': 'meeting_rooms',\n",
    "        'conference': 'meeting_rooms',\n",
    "        'printer': 'printing',\n",
    "        'print': 'printing',\n",
    "        'rooftop': 'rooftop',\n",
    "        'terrace': 'rooftop',\n",
    "        '24/7': '24/7_access',\n",
    "        '24h': '24/7_access',\n",
    "        'lounge': 'lounge',\n",
    "        'event': 'events',\n",
    "        'workspace': 'dedicated_desk',\n",
    "        'dedicated desk': 'dedicated_desk'\n",
    "    }\n",
    "    for keyword, amenity in amenity_keywords.items():\n",
    "        if keyword in description and amenity in amenities_list:\n",
    "            found_amenities.append(amenity)\n",
    "    return list(set(found_amenities))\n",
    "\n",
    "# Function to detect sequential price patterns like \"1 2 3 4\"\n",
    "def is_sequential_price(price):\n",
    "    if pd.isna(price):\n",
    "        return False\n",
    "    \n",
    "    # Convert the price to string for pattern matching\n",
    "    price_str = str(price).strip()\n",
    "    \n",
    "    # Check for sequential numbers with spaces between them\n",
    "    seq_pattern = re.compile(r'\\b\\d+\\s+\\d+\\s+\\d+\\b')\n",
    "    if seq_pattern.search(price_str):\n",
    "        return True\n",
    "    \n",
    "    # Also check for any price that has more than 3 numbers separated by spaces\n",
    "    if len(re.findall(r'\\b\\d+\\b', price_str)) > 3:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "current_dir = os.getcwd()\n",
    "amenities_file_path = os.path.join(current_dir, \"src\", \"results\", \"extracted_amenities.csv\")\n",
    "coworking_file_path = os.path.join(current_dir, \"src\", \"results\", \"merged_coworking_spaces.csv\")\n",
    "\n",
    "# Check if files exist and create directories if needed\n",
    "os.makedirs(os.path.join(current_dir, \"src\", \"results\"), exist_ok=True)\n",
    "\n",
    "# Check if both files exist, if not, use sample data for demonstration\n",
    "if not os.path.exists(amenities_file_path) or not os.path.exists(coworking_file_path):\n",
    "    print(\"Warning: Source files not found. Creating sample data for demonstration.\")\n",
    "    \n",
    "    # Create sample coworking data\n",
    "    coworking_data = {\n",
    "        'name': [f'Coworking Space {i}' for i in range(1, 101)],\n",
    "        'city': np.random.choice(['Berlin', 'Paris', 'London', 'Madrid', 'Rome'], 100),\n",
    "        'address': [f'{i} Main Street' for i in range(1, 101)],\n",
    "        'price': [f\"€{np.random.randint(10, 50)}\" for _ in range(100)],\n",
    "        'description': [\n",
    "            f\"A {'modern' if i % 2 == 0 else 'cozy'} coworking space with {'wifi' if i % 3 == 0 else ''} \" + \n",
    "            f\"{'coffee' if i % 4 == 0 else ''} {'meeting rooms' if i % 5 == 0 else ''}\"\n",
    "            for i in range(1, 101)\n",
    "        ],\n",
    "        'url': [f'https://example.com/space{i}' for i in range(1, 101)]\n",
    "    }\n",
    "    \n",
    "    # Create sample amenities data\n",
    "    amenities_data = {\n",
    "        'extracted_amenities': [\n",
    "            str(list(np.random.choice(\n",
    "                ['wifi', 'coffee', 'kitchen', 'parking', 'bike_storage', 'locker', \n",
    "                 'meeting_rooms', 'printing', 'rooftop', '24/7_access', 'lounge', 'events', \n",
    "                 'dedicated_desk'], \n",
    "                size=np.random.randint(1, 8), \n",
    "                replace=False\n",
    "            )))\n",
    "            for _ in range(100)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrames\n",
    "    coworking_df = pd.DataFrame(coworking_data)\n",
    "    amenities_df = pd.DataFrame(amenities_data)\n",
    "    \n",
    "    # Save the sample data\n",
    "    coworking_df.to_csv(coworking_file_path, index=False)\n",
    "    amenities_df.to_csv(amenities_file_path, index=False)\n",
    "    \n",
    "    print(f\"Created sample files at:\\n{coworking_file_path}\\n{amenities_file_path}\")\n",
    "else:\n",
    "    print(\"Loading existing data files...\")\n",
    "\n",
    "# Load datasets\n",
    "amenities_df = pd.read_csv(amenities_file_path)\n",
    "coworking_df = pd.read_csv(coworking_file_path)\n",
    "\n",
    "print(f\"Loaded amenities data: {amenities_df.shape}\")\n",
    "print(f\"Loaded coworking data: {coworking_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representation of lists to actual lists\n",
    "amenities_df['amenities_list'] = amenities_df['extracted_amenities'].apply(parse_amenities)\n",
    "\n",
    "# Merge the dataframes\n",
    "if len(amenities_df) == len(coworking_df):\n",
    "    df = pd.concat([coworking_df, amenities_df['amenities_list']], axis=1)\n",
    "else:\n",
    "    amenities_df = amenities_df.reset_index()\n",
    "    coworking_df = coworking_df.reset_index()\n",
    "    df = pd.merge(coworking_df, amenities_df[['index', 'amenities_list']], on='index', how='left')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract All Unique Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique amenities from the amenities lists\n",
    "all_amenities = []\n",
    "for amenities in df['amenities_list']:\n",
    "    if isinstance(amenities, list):\n",
    "        all_amenities.extend(amenities)\n",
    "all_unique_amenities = list(set(all_amenities))\n",
    "\n",
    "print(f\"Found {len(all_unique_amenities)} unique amenities:\")\n",
    "print(all_unique_amenities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhance Amenities with Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance amenities with descriptions\n",
    "if 'description' in df.columns:\n",
    "    for index, row in df.iterrows():\n",
    "        desc_amenities = find_amenities_in_description(row.get('description', ''), all_unique_amenities)\n",
    "        current_amenities = row['amenities_list'] if isinstance(row['amenities_list'], list) else []\n",
    "        combined_amenities = list(set(current_amenities + desc_amenities))\n",
    "        df.at[index, 'amenities_list'] = combined_amenities\n",
    "    \n",
    "    # Count amenities after enhancement\n",
    "    all_amenities = []\n",
    "    for amenities in df['amenities_list']:\n",
    "        if isinstance(amenities, list):\n",
    "            all_amenities.extend(amenities)\n",
    "            \n",
    "    # Get the most common amenities\n",
    "    amenity_counts = Counter(all_amenities)\n",
    "    most_common_amenities = amenity_counts.most_common(15)\n",
    "    \n",
    "    print(\"Most common amenities after enhancement:\")\n",
    "    for amenity, count in most_common_amenities:\n",
    "        print(f\"{amenity}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Amenity Columns for Filtering and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create amenity columns for filtering and clustering\n",
    "for amenity in set(all_amenities):\n",
    "    df[f'has_amenity_{amenity}'] = df['amenities_list'].apply(\n",
    "        lambda x: 1 if isinstance(x, list) and amenity in x else 0\n",
    "    )\n",
    "\n",
    "# Process price information if available\n",
    "if 'price' in df.columns:\n",
    "    # Mark invalid sequential prices\n",
    "    df['is_invalid_price'] = df['price'].apply(is_sequential_price)\n",
    "    \n",
    "    # Extract numeric prices only for valid prices\n",
    "    df['price_numeric'] = np.nan  # Initialize with NaN\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if not row['is_invalid_price']:\n",
    "            # Extract numeric values from valid price strings\n",
    "            if not pd.isna(row['price']):\n",
    "                price_match = re.search(r'(\\d+(?:\\.\\d+)?)', str(row['price']))\n",
    "                if price_match:\n",
    "                    df.at[idx, 'price_numeric'] = float(price_match.group(1))\n",
    "\n",
    "# Display the processed dataframe\n",
    "df.iloc[:5, :10]  # Display first 5 rows and 10 columns for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement the Clustering Model from Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build clustering model function from the app\n",
    "def build_clustering_model(df, n_clusters=5):\n",
    "    # Remove outliers based on price using the IQR method\n",
    "    if 'price_numeric' in df.columns:\n",
    "        Q1 = df['price_numeric'].quantile(0.25)\n",
    "        Q3 = df['price_numeric'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_filtered = df[(df['price_numeric'] >= lower_bound) & (df['price_numeric'] <= upper_bound)].copy()\n",
    "        # Reset index to avoid index matching issues\n",
    "        df_filtered = df_filtered.reset_index(drop=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "    else:\n",
    "        df_filtered = df.copy()\n",
    "        df_filtered = df_filtered.reset_index(drop=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Get amenity columns\n",
    "    amenity_cols = [col for col in df.columns if col.startswith('has_amenity_')]\n",
    "    if not amenity_cols:\n",
    "        return None, None, None\n",
    "\n",
    "    # Create feature matrix from filtered dataframe\n",
    "    X = df_filtered[amenity_cols].fillna(0)\n",
    "\n",
    "    # Add price as a normalized feature if available\n",
    "    if 'price_numeric' in df.columns:\n",
    "        scaler = StandardScaler()\n",
    "        price_scaled = scaler.fit_transform(\n",
    "            df_filtered[['price_numeric']].fillna(df_filtered['price_numeric'].mean()).values\n",
    "        )\n",
    "        X['price_scaled'] = price_scaled\n",
    "\n",
    "    # Fit KMeans on the filtered dataset\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Create an array for cluster assignments for the entire dataframe\n",
    "    clusters = -1 * np.ones(len(df), dtype=int)\n",
    "    clusters[df_filtered.index] = cluster_labels\n",
    "\n",
    "    # Compute cluster centers using only the filtered data features\n",
    "    cluster_centers = pd.DataFrame(\n",
    "        kmeans.cluster_centers_,\n",
    "        columns=X.columns\n",
    "    )\n",
    "\n",
    "    return clusters, cluster_centers, kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Elbow Method to Find Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get amenity columns\n",
    "amenity_cols = [col for col in df.columns if col.startswith('has_amenity_')]\n",
    "X = df[amenity_cols].fillna(0)\n",
    "\n",
    "# Add price as a normalized feature if available\n",
    "if 'price_numeric' in df.columns:\n",
    "    scaler = StandardScaler()\n",
    "    price_scaled = scaler.fit_transform(\n",
    "        df[['price_numeric']].fillna(df['price_numeric'].mean()).values\n",
    "    )\n",
    "    X['price_scaled'] = price_scaled\n",
    "\n",
    "# Calculate SSE for different values of k\n",
    "sse = []\n",
    "# Adjust k_range based on dataset size\n",
    "max_k = min(10, len(df) - 1)\n",
    "k_range = range(2, max_k + 1)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, sse, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find elbow point using the angle method\n",
    "deltas = np.diff(sse)\n",
    "angles = np.diff(deltas)\n",
    "elbow_point = np.argmin(angles) + 2\n",
    "print(f\"Suggested optimal number of clusters: {elbow_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build and Analyze the Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the optimal number of clusters from elbow method\n",
    "n_clusters = elbow_point\n",
    "print(f\"Building clustering model with {n_clusters} clusters...\")\n",
    "\n",
    "# Build the clustering model\n",
    "clusters, cluster_centers, kmeans_model = build_clustering_model(df, n_clusters)\n",
    "\n",
    "if clusters is not None:\n",
    "    df['cluster'] = clusters\n",
    "    print(f\"Clustered {len(df)} coworking spaces into {n_clusters} clusters\")\n",
    "    \n",
    "    # Compute cluster characteristics\n",
    "    cluster_characteristics = {}\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_df = df[df['cluster'] == cluster]\n",
    "        size = len(cluster_df)\n",
    "        avg_price = cluster_df['price_numeric'].mean() if 'price_numeric' in cluster_df.columns and size > 0 else None\n",
    "        \n",
    "        # Get amenity columns\n",
    "        amenity_cols = [col for col in df.columns if col.startswith('has_amenity_')]\n",
    "        amenity_counts = {}\n",
    "        for col in amenity_cols:\n",
    "            # Sum the column for the cluster\n",
    "            count = cluster_df[col].sum()\n",
    "            if count > 0:\n",
    "                # Format amenity name\n",
    "                amenity = col.replace('has_amenity_', '').replace('_', ' ').title()\n",
    "                amenity_counts[amenity] = count\n",
    "        \n",
    "        # Top 3 amenities sorted by occurrence\n",
    "        top_amenities = sorted(amenity_counts, key=amenity_counts.get, reverse=True)[:3]\n",
    "        \n",
    "        cluster_characteristics[cluster] = {\n",
    "            'size': size,\n",
    "            'avg_price': avg_price,\n",
    "            'top_amenities': top_amenities\n",
    "        }\n",
    "        \n",
    "    # Display cluster characteristics\n",
    "    for cluster, chars in cluster_characteristics.items():\n",
    "        print(f\"\\nCluster {cluster} ({chars['size']} spaces):\")\n",
    "        print(\"  Top Amenities:\")\n",
    "        for amenity in chars['top_amenities']:\n",
    "            print(f\"    - {amenity}\")\n",
    "        \n",
    "        if chars['avg_price'] is not None:\n",
    "            print(f\"  Average Price: ${chars['avg_price']:.2f}\")\n",
    "else:\n",
    "    print(\"Could not build clusters with the current data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clusters is not None and 'price_numeric' in df.columns:\n",
    "    # Create scatter plot of prices vs amenity count by cluster\n",
    "    viz_df = df.copy()\n",
    "    amenity_cols = [col for col in viz_df.columns if col.startswith('has_amenity_')]\n",
    "    viz_df['amenity_count'] = viz_df[amenity_cols].sum(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_data = viz_df[viz_df['cluster'] == cluster]\n",
    "        plt.scatter(\n",
    "            cluster_data['price_numeric'], \n",
    "            cluster_data['amenity_count'],\n",
    "            alpha=0.7,\n",
    "            label=f'Cluster {cluster}'\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Price')\n",
    "    plt.ylabel('Number of Amenities')\n",
    "    plt.title('Clusters by Price and Amenity Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show amenity distribution across clusters using heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(\n",
    "        cluster_centers.iloc[:, cluster_centers.columns.str.startswith('has_amenity_')],\n",
    "        cmap='YlOrRd',\n",
    "        annot=True,\n",
    "        fmt=\".2f\"\n",
    "    )\n",
    "    plt.title('Amenity Distribution by Cluster')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model if it was successfully built\n",
    "if kmeans_model is not None:\n",
    "    # Create a dictionary with all model components\n",
    "    model_package = {\n",
    "        'kmeans_model': kmeans_model,\n",
    "        'n_clusters': n_clusters,\n",
    "        'feature_columns': [col for col in df.columns if col.startswith('has_amenity_')],\n",
    "        'cluster_centers': cluster_centers,\n",
    "        'all_unique_amenities': all_unique_amenities\n",
    "    }\n",
    "    \n",
    "    # Save the model package\n",
    "    with open('coworking_clustering_model.pkl', 'wb') as file:\n",
    "        pickle.dump(model_package, file)\n",
    "    \n",
    "    print(\"Clustering model saved as 'coworking_clustering_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Cleaned Data as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a cleaned version of the data with cluster assignments\n",
    "cleaned_df = df.copy()\n",
    "\n",
    "# Drop any unwanted columns\n",
    "columns_to_drop = [col for col in cleaned_df.columns if col.startswith('has_amenity_')]\n",
    "columns_to_drop.extend(['is_invalid_price', 'index'])\n",
    "cleaned_df = cleaned_df.drop(columns=[col for col in columns_to_drop if col in cleaned_df.columns])\n",
    "\n",
    "# Function to convert DataFrame to a serializable format\n",
    "def df_to_serializable(df):\n",
    "    result = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Convert row to dict and handle non-serializable values\n",
    "        row_dict = {}\n",
    "        for col, val in row.items():\n",
    "            if pd.isna(val):\n",
    "                row_dict[col] = None\n",
    "            elif isinstance(val, np.int64):\n",
    "                row_dict[col] = int(val)\n",
    "            elif isinstance(val, np.float64):\n",
    "                row_dict[col] = float(val)\n",
    "            else:\n",
    "                row_dict[col] = val\n",
    "        result.append(row_dict)\n",
    "    return result\n",
    "\n",
    "# Convert to serializable format and save as JSON\n",
    "serializable_data = df_to_serializable(cleaned_df)\n",
    "with open('coworking_spaces_cleaned.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(serializable_data, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(\"Cleaned data saved as 'coworking_spaces_cleaned.json'\")\n",
  "print(f\"Total records saved: {len(serializable_data)}\")"
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The clustering model has been successfully built and saved, along with the cleaned data. This model can now be used for further analysis or integration into applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
